{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple MLP demo\n",
    "\n",
    "This notebook demonstrates how to create a simple MLP for recognizing phonemes from speech. To do this, we will use a training dataset prepared in a different notebook titled *VoxforgeDataPrep*, so take a look at that before you start working on this demo.\n",
    "\n",
    "In this example, we will use the excellent [Keras](http://keras.io/) library which depends upon either Theano or TensorFlow, so you will need to install those as well. Just follow the isntructions on the Keras website - it is recommended to use the freshest, Github versions of both Keras and Theano.\n",
    "\n",
    "I also have the convinence of using the GPU for the actual computation. This code will work just as well on the CPU, but it's much faster on a good GPU.\n",
    "\n",
    "We start by importing numpy (for loading and working with the data) and the neccessary Keras classes. Feel free to add more here if you wish to experiment with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN (CNMeM is enabled)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD, Adadelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's load our data. In the *VoxforgeDataPrep* notebook, we created to arrays - inputs and outputs. The input nas the dimensions (num_samples,num_features) and the output is simply 1D vector of ints of length (num_samples). In this step, we split the data into training (80%), dev (10%) and test (10%) portions and save the indices referencing these parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs=np.load('../data/mfcc_in_norm.npy')\n",
    "output_dec=np.load('../data/mfcc_out.npy')\n",
    "\n",
    "num=inputs.shape[0]\n",
    "\n",
    "p1=int(num*0.8)\n",
    "p2=int(num*0.9)\n",
    "\n",
    "train_idx=range(0,p1)\n",
    "dev_idx=range(p1,p2)\n",
    "test_idx=range(p2,num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the context sensitivity of the model, we take 7 samples (3 before and 3 after) from the input and provide them to the network all at once. This increases our input size from 39 to 273 elements, which is still managable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def expandTimeWindow(data, time_before, time_after):\n",
    "\n",
    "    newdata = []\n",
    "\n",
    "    last_win = data.shape[0] - 1\n",
    "\n",
    "    for w in range(data.shape[0]):\n",
    "\n",
    "        newwin = []\n",
    "        for x in range(w - time_before, w + time_after + 1):\n",
    "            w = x\n",
    "            if(w < 0):\n",
    "                w = 0\n",
    "            if(w > last_win):\n",
    "                w = last_win\n",
    "            newwin.append(data[w])\n",
    "        newdata.append(np.concatenate(newwin))\n",
    "\n",
    "    return np.vstack(newdata)\n",
    "\n",
    "inputs=expandTimeWindow(inputs,3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define some constants for our program. Input and output dimensions can be inferred from the data, but the hidden layer size has to be defined manually.\n",
    "\n",
    "We also redefine our outputs as a 1-of-N matrix instead of an int vector. The old outputs were simply a list of integers (from 0 to 39) defining the phoneme (as listed in ../data/phones.list) class for each sample given at input. The new matrix has dimensions (num_samples, num_classes) and is mostly 0 with a single 1 put in place corresponding to the class index in the old output vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_dim=inputs.shape[1]\n",
    "output_dim=np.max(output_dec)+1\n",
    "\n",
    "hidden_num=256\n",
    "\n",
    "outputs=np.zeros((num,output_dim))\n",
    "outputs[range(0,num),output_dec]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples num: 1438610\n",
      "Input size: 273\n",
      "Output size (number of classes): 40\n"
     ]
    }
   ],
   "source": [
    "print 'Samples num: {}'.format(num)\n",
    "print 'Input size: {}'.format(input_dim)\n",
    "print 'Output size (number of classes): {}'.format(output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition\n",
    "\n",
    "Here we define our model using the Keras interface. There are two main model types in Keras: sequential and graph. Sequential is much more common and easy to use, so we start with that.\n",
    "\n",
    "Next we define the MLP topology. Here we have 3 layers: input, hidden and output. They are interconnected with two sets of *Dense* weight connections and a layer of activation functions after these weights. When defining the *Dense* weight layers, we need to provide the size: input and output are neccessary only for the first layer, subsequent layers use the output size of the previous layer as their input size.\n",
    "\n",
    "We also define the type of optimizer and loss function we want to use. There are a few optimizers to choose from in the library and they are all interchangable. The differences between them are not too large in this example (feel free to experiment). The loss function chosen here is the cross-entropy function. Another option would be the simpler MSE (mean square error). Again, there doesn't seem to be much of a difference, but cross-entropy does seem like performing a bit better overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(input_dim=input_dim,output_dim=hidden_num))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(output_dim=output_dim))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#optimizer = SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "optimizer= Adadelta()\n",
    "loss='categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the model and all its parameters, we can compile it. This literally means compiling, because the model is converted into C++ code in the background and compiled with lots of optimizations to work as efficiently as possible. The process can take a while, but is worth the added speed in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(loss=loss, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can start training the model. We provide the training function both training and validation data and define a few parameters: batch size and number of training epochs. Changing the batch size can affect both the training speed and final accuracy. This value is also closely related to the number of epochs. Generally, you want to run the training for as many epochs as needed for the model to converge on some value. The value of 100 should be fine for a quick comparison but up to 1k may be necessary to be abolutely sure (especially when testing larger models).\n",
    "\n",
    "Setting the verbose value to 2 makes more sense in notebooks, but leaving it at one creates a nice progress bar if you run the program from the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1150888 samples, validate on 143861 samples\n",
      "Epoch 1/100\n",
      "5s - loss: 1.5383 - val_loss: 1.4148\n",
      "Epoch 2/100\n",
      "5s - loss: 1.2941 - val_loss: 1.2930\n",
      "Epoch 3/100\n",
      "5s - loss: 1.2130 - val_loss: 1.2277\n",
      "Epoch 4/100\n",
      "5s - loss: 1.1694 - val_loss: 1.2165\n",
      "Epoch 5/100\n",
      "5s - loss: 1.1410 - val_loss: 1.2018\n",
      "Epoch 6/100\n",
      "5s - loss: 1.1208 - val_loss: 1.1821\n",
      "Epoch 7/100\n",
      "5s - loss: 1.1050 - val_loss: 1.1629\n",
      "Epoch 8/100\n",
      "5s - loss: 1.0920 - val_loss: 1.1483\n",
      "Epoch 9/100\n",
      "5s - loss: 1.0814 - val_loss: 1.1693\n",
      "Epoch 10/100\n",
      "5s - loss: 1.0722 - val_loss: 1.1468\n",
      "Epoch 11/100\n",
      "5s - loss: 1.0648 - val_loss: 1.1589\n",
      "Epoch 12/100\n",
      "5s - loss: 1.0580 - val_loss: 1.1478\n",
      "Epoch 13/100\n",
      "5s - loss: 1.0519 - val_loss: 1.1387\n",
      "Epoch 14/100\n",
      "5s - loss: 1.0464 - val_loss: 1.1328\n",
      "Epoch 15/100\n",
      "5s - loss: 1.0416 - val_loss: 1.1364\n",
      "Epoch 16/100\n",
      "5s - loss: 1.0370 - val_loss: 1.1306\n",
      "Epoch 17/100\n",
      "5s - loss: 1.0334 - val_loss: 1.1353\n",
      "Epoch 18/100\n",
      "5s - loss: 1.0295 - val_loss: 1.1327\n",
      "Epoch 19/100\n",
      "5s - loss: 1.0266 - val_loss: 1.1258\n",
      "Epoch 20/100\n",
      "5s - loss: 1.0233 - val_loss: 1.1244\n",
      "Epoch 21/100\n",
      "5s - loss: 1.0200 - val_loss: 1.1268\n",
      "Epoch 22/100\n",
      "5s - loss: 1.0176 - val_loss: 1.1350\n",
      "Epoch 23/100\n",
      "5s - loss: 1.0153 - val_loss: 1.1380\n",
      "Epoch 24/100\n",
      "5s - loss: 1.0131 - val_loss: 1.1242\n",
      "Epoch 25/100\n",
      "5s - loss: 1.0108 - val_loss: 1.1306\n",
      "Epoch 26/100\n",
      "5s - loss: 1.0088 - val_loss: 1.1106\n",
      "Epoch 27/100\n",
      "5s - loss: 1.0071 - val_loss: 1.1122\n",
      "Epoch 28/100\n",
      "5s - loss: 1.0050 - val_loss: 1.1246\n",
      "Epoch 29/100\n",
      "5s - loss: 1.0029 - val_loss: 1.1051\n",
      "Epoch 30/100\n",
      "5s - loss: 1.0013 - val_loss: 1.1266\n",
      "Epoch 31/100\n",
      "5s - loss: 1.0001 - val_loss: 1.1092\n",
      "Epoch 32/100\n",
      "5s - loss: 0.9985 - val_loss: 1.1070\n",
      "Epoch 33/100\n",
      "5s - loss: 0.9970 - val_loss: 1.1219\n",
      "Epoch 34/100\n",
      "5s - loss: 0.9957 - val_loss: 1.1310\n",
      "Epoch 35/100\n",
      "5s - loss: 0.9945 - val_loss: 1.1117\n",
      "Epoch 36/100\n",
      "5s - loss: 0.9931 - val_loss: 1.1110\n",
      "Epoch 37/100\n",
      "5s - loss: 0.9920 - val_loss: 1.1240\n",
      "Epoch 38/100\n",
      "5s - loss: 0.9908 - val_loss: 1.1059\n",
      "Epoch 39/100\n",
      "5s - loss: 0.9896 - val_loss: 1.1088\n",
      "Epoch 40/100\n",
      "5s - loss: 0.9883 - val_loss: 1.0961\n",
      "Epoch 41/100\n",
      "5s - loss: 0.9876 - val_loss: 1.1160\n",
      "Epoch 42/100\n",
      "5s - loss: 0.9866 - val_loss: 1.1048\n",
      "Epoch 43/100\n",
      "5s - loss: 0.9857 - val_loss: 1.1140\n",
      "Epoch 44/100\n",
      "5s - loss: 0.9846 - val_loss: 1.1164\n",
      "Epoch 45/100\n",
      "5s - loss: 0.9836 - val_loss: 1.0916\n",
      "Epoch 46/100\n",
      "5s - loss: 0.9827 - val_loss: 1.1145\n",
      "Epoch 47/100\n",
      "5s - loss: 0.9819 - val_loss: 1.0978\n",
      "Epoch 48/100\n",
      "5s - loss: 0.9811 - val_loss: 1.0948\n",
      "Epoch 49/100\n",
      "5s - loss: 0.9804 - val_loss: 1.0961\n",
      "Epoch 50/100\n",
      "5s - loss: 0.9796 - val_loss: 1.0968\n",
      "Epoch 51/100\n",
      "5s - loss: 0.9788 - val_loss: 1.1036\n",
      "Epoch 52/100\n",
      "5s - loss: 0.9783 - val_loss: 1.0942\n",
      "Epoch 53/100\n",
      "5s - loss: 0.9776 - val_loss: 1.0965\n",
      "Epoch 54/100\n",
      "5s - loss: 0.9767 - val_loss: 1.0930\n",
      "Epoch 55/100\n",
      "5s - loss: 0.9758 - val_loss: 1.1049\n",
      "Epoch 56/100\n",
      "5s - loss: 0.9756 - val_loss: 1.1035\n",
      "Epoch 57/100\n",
      "5s - loss: 0.9749 - val_loss: 1.1185\n",
      "Epoch 58/100\n",
      "5s - loss: 0.9744 - val_loss: 1.0935\n",
      "Epoch 59/100\n",
      "5s - loss: 0.9735 - val_loss: 1.1011\n",
      "Epoch 60/100\n",
      "5s - loss: 0.9730 - val_loss: 1.1044\n",
      "Epoch 61/100\n",
      "5s - loss: 0.9722 - val_loss: 1.0979\n",
      "Epoch 62/100\n",
      "5s - loss: 0.9718 - val_loss: 1.1027\n",
      "Epoch 63/100\n",
      "5s - loss: 0.9712 - val_loss: 1.1020\n",
      "Epoch 64/100\n",
      "5s - loss: 0.9708 - val_loss: 1.0956\n",
      "Epoch 65/100\n",
      "5s - loss: 0.9703 - val_loss: 1.1008\n",
      "Epoch 66/100\n",
      "5s - loss: 0.9696 - val_loss: 1.0948\n",
      "Epoch 67/100\n",
      "5s - loss: 0.9690 - val_loss: 1.1022\n",
      "Epoch 68/100\n",
      "5s - loss: 0.9686 - val_loss: 1.1231\n",
      "Epoch 69/100\n",
      "5s - loss: 0.9683 - val_loss: 1.0944\n",
      "Epoch 70/100\n",
      "5s - loss: 0.9676 - val_loss: 1.1005\n",
      "Epoch 71/100\n",
      "5s - loss: 0.9673 - val_loss: 1.1003\n",
      "Epoch 72/100\n",
      "5s - loss: 0.9666 - val_loss: 1.0863\n",
      "Epoch 73/100\n",
      "5s - loss: 0.9664 - val_loss: 1.1068\n",
      "Epoch 74/100\n",
      "5s - loss: 0.9658 - val_loss: 1.1029\n",
      "Epoch 75/100\n",
      "5s - loss: 0.9652 - val_loss: 1.0915\n",
      "Epoch 76/100\n",
      "5s - loss: 0.9650 - val_loss: 1.0957\n",
      "Epoch 77/100\n",
      "5s - loss: 0.9645 - val_loss: 1.0932\n",
      "Epoch 78/100\n",
      "5s - loss: 0.9638 - val_loss: 1.0994\n",
      "Epoch 79/100\n",
      "5s - loss: 0.9640 - val_loss: 1.0897\n",
      "Epoch 80/100\n",
      "5s - loss: 0.9634 - val_loss: 1.0857\n",
      "Epoch 81/100\n",
      "5s - loss: 0.9630 - val_loss: 1.0985\n",
      "Epoch 82/100\n",
      "5s - loss: 0.9625 - val_loss: 1.0925\n",
      "Epoch 83/100\n",
      "5s - loss: 0.9622 - val_loss: 1.0835\n",
      "Epoch 84/100\n",
      "5s - loss: 0.9618 - val_loss: 1.0926\n",
      "Epoch 85/100\n",
      "5s - loss: 0.9618 - val_loss: 1.0838\n",
      "Epoch 86/100\n",
      "5s - loss: 0.9608 - val_loss: 1.0870\n",
      "Epoch 87/100\n",
      "5s - loss: 0.9608 - val_loss: 1.0926\n",
      "Epoch 88/100\n",
      "5s - loss: 0.9605 - val_loss: 1.0898\n",
      "Epoch 89/100\n",
      "5s - loss: 0.9600 - val_loss: 1.0871\n",
      "Epoch 90/100\n",
      "5s - loss: 0.9593 - val_loss: 1.0975\n",
      "Epoch 91/100\n",
      "6s - loss: 0.9594 - val_loss: 1.0817\n",
      "Epoch 92/100\n",
      "6s - loss: 0.9591 - val_loss: 1.0938\n",
      "Epoch 93/100\n",
      "5s - loss: 0.9588 - val_loss: 1.0926\n",
      "Epoch 94/100\n",
      "5s - loss: 0.9584 - val_loss: 1.0993\n",
      "Epoch 95/100\n",
      "5s - loss: 0.9584 - val_loss: 1.0979\n",
      "Epoch 96/100\n",
      "5s - loss: 0.9581 - val_loss: 1.0957\n",
      "Epoch 97/100\n",
      "5s - loss: 0.9575 - val_loss: 1.0864\n",
      "Epoch 98/100\n",
      "5s - loss: 0.9576 - val_loss: 1.0843\n",
      "Epoch 99/100\n",
      "5s - loss: 0.9570 - val_loss: 1.0939\n",
      "Epoch 100/100\n",
      "5s - loss: 0.9568 - val_loss: 1.0804\n"
     ]
    }
   ],
   "source": [
    "val=(inputs[dev_idx],outputs[dev_idx])\n",
    "\n",
    "hist=model.fit(inputs[train_idx], outputs[train_idx], batch_size=256, nb_epoch=100, verbose=2, validation_data=val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training method returns an object that contains the trained model parameters and the training history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdddfe3bd90>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEACAYAAAC57G0KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdddff0ed50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as P\n",
    "%matplotlib inline\n",
    "\n",
    "P.plot(hist.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get better graphs and more data if you overload the training callback method, which will provide you with the model parameters after each epoch during training.\n",
    "\n",
    "After the model is trained, we can easily test it using the evaluate method. The show_accuracy argument is required to compute the accuracy of the decision variable. The returned result has a 2-element list, where the first value is the loss of the model on the test data and the second is the accuracy, around 70% in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res=model.evaluate(inputs[test_idx],outputs[test_idx],batch_size=256,show_accuracy=True,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.00941736841\n",
      "Accuracy: 69.926526%\n"
     ]
    }
   ],
   "source": [
    "print 'Loss: {}'.format(res[0])\n",
    "print 'Accuracy: {:%}'.format(res[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Further steps\n",
    "\n",
    "You can play around with the different parameters and network topologies. The results aren't going to be much better using this type of model. Using recurrent topologies (e.g. LSTM) can work better, as well as providing more data. Crucially, however, framewise phoneme classification is not the best benchmark to test and isn't the most useful. Further notebooks will go into other technuiques for getting closer to the best speech recognition can provide."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
