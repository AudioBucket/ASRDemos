{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theano implementation of HMM algorithms\n",
    "\n",
    "This notebook implements some of the HMM algorithms using Theano. These may be helpful for incorporating them in the graphs of some other models in Theano.\n",
    "\n",
    "## Discrete HMM\n",
    "\n",
    "We start with a toy HMM example from [Rabiner's](http://www.ee.columbia.edu/~dpwe/e6820/papers/Rabiner89-hmm.pdf) paper.\n",
    "\n",
    "Many improvements can be added to this (and may be added in the future):\n",
    "  \n",
    "  * everything should be moved to the log-domain - for computation stability. This is especially important for GPUs which usually have a lower floating-point precision. This example here works only for the smallest examples (a few observations).\n",
    "  \n",
    "  * continous density models - this discrete example cannot be applied to most practical examples.\n",
    "  \n",
    "  * custom topologies - this is a simple ergodic example. In practice, we'd want to be able to design a specific transition graph for a particular use.\n",
    "  \n",
    "  * combining models - real applications rely on being able to combine several models to be able to train them for specific problems. E.g. combining many tri-phone models for speech recognition. This should also include the ability to do state-tying.\n",
    "  \n",
    "  * better training algorithms - only Baum-Welch and the simplest gradient descent are demonstrated here. There are other methods that oculd work better/faster.\n",
    "  \n",
    "The example below will demonstrate the code using a similar notation and equations from Rabiner's paper. First we import the required stuff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import pickle\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HMM class code\n",
    "\n",
    "Here we create the HMM class. Everything is compiled in the constructor. We also provide methods for all the individual algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DiscreteHMM:\n",
    "    \n",
    "    def __init__(self, N=3, M=4):        \n",
    "        \n",
    "        updates={}\n",
    "                \n",
    "        pi = theano.shared((np.ones(N)/N).astype(theano.config.floatX))\n",
    "        a = theano.shared((np.ones((N,N))/(N*np.ones(N))).astype(theano.config.floatX))\n",
    "        b = theano.shared((np.ones((N,M))/(N*np.ones(M))).astype(theano.config.floatX))\n",
    "        \n",
    "        self.pi=pi\n",
    "        self.a=a\n",
    "        self.b=b\n",
    "        \n",
    "        O = T.ivector()\n",
    "        TT = O.shape[0]\n",
    "        \n",
    "        \n",
    "        #forward algorithm:\n",
    "        \n",
    "        alpha0=pi*b[:,O[0]]\n",
    "        \n",
    "        alpha_scan,upd = theano.scan(fn=lambda O,alpha_p: T.dot(alpha_p,a)*b[:,O],\n",
    "                               sequences=O[1:],\n",
    "                               outputs_info=alpha0)\n",
    "        \n",
    "        updates.update(upd)\n",
    "        \n",
    "        alpha=T.concatenate((alpha0.reshape((1,N)),alpha_scan))                \n",
    "        \n",
    "        #backward algorithm:\n",
    "        \n",
    "        beta0=T.ones(N).astype(theano.config.floatX)\n",
    "        \n",
    "        beta_scan,upd = theano.scan(fn=lambda O,beta_p: T.dot(beta_p*b[:,O],a.T),\n",
    "                               sequences=O[1:],\n",
    "                               outputs_info=beta0,\n",
    "                               go_backwards=True)\n",
    "        updates.update(upd)\n",
    "        \n",
    "        beta=T.concatenate((beta_scan[::-1],beta0.reshape((1,N))))        \n",
    "        \n",
    "        #full model probability:\n",
    "        \n",
    "        full_prob = alpha_scan[-1].sum()        \n",
    "        \n",
    "        #forward-backward probabilities:\n",
    "        \n",
    "        gamma=alpha*beta/full_prob        \n",
    "        \n",
    "        #viterbi algorithm:\n",
    "        \n",
    "        def viterbi_rec_step(O, delta_p, phi_p):\n",
    "            m=delta_p*a.T\n",
    "            phi=m.argmax(axis=1)\n",
    "            delta=m[T.arange(N),phi]*b[:,O]\n",
    "            return delta,phi\n",
    "        \n",
    "        phi0=T.zeros(N).astype('int64')\n",
    "\n",
    "        [delta_scan, phi_scan], upd = theano.scan(fn=viterbi_rec_step,\n",
    "                                                  sequences=O[1:],\n",
    "                                                  outputs_info=[alpha0,phi0])        \n",
    "        \n",
    "        updates.update(upd)\n",
    "        \n",
    "        QT=phi_scan[-1].argmax()        \n",
    "        vite_prob = delta_scan[-1,QT]\n",
    "        \n",
    "        Q_scan, upd = theano.scan(fn=lambda phi, Q: phi[Q],\n",
    "                             sequences=phi_scan,\n",
    "                             outputs_info=QT,\n",
    "                             go_backwards=True)\n",
    "        \n",
    "        updates.update(upd)\n",
    "                                                  \n",
    "        Q=T.concatenate((Q_scan[::-1],QT.reshape((1,))))\n",
    "        \n",
    "        #transition probabilities\n",
    "        \n",
    "        xi=alpha[:-1].reshape((TT-1,N,1))*a.reshape((1,N,N))*b[:,O[1:]].T.reshape((TT-1,1,N))*beta[1:].reshape((TT-1,1,N))/full_prob\n",
    "        \n",
    "        #expected values\n",
    "        \n",
    "        exp_pi=gamma[0]\n",
    "        \n",
    "        exp_a=xi.sum(axis=0)/gamma[:-1].sum(axis=0).reshape((N,1))\n",
    "        \n",
    "        exp_b_map, upd = theano.map(fn=lambda k: T.sum(gamma[T.eq(O,k).nonzero()],axis=0)/T.sum(gamma,axis=0), \n",
    "                         sequences=T.arange(M))\n",
    "        \n",
    "        updates.update(upd)\n",
    "        \n",
    "        exp_b = exp_b_map.T\n",
    "        \n",
    "        exp_err = T.concatenate(((pi-exp_pi).ravel(),(a-exp_a).ravel(),(b-exp_b).ravel()))\n",
    "        \n",
    "        exp_mean_err = T.mean(exp_err**2)\n",
    "        \n",
    "        #Baum-Welch updates:\n",
    "        \n",
    "        baum_welch_updates=OrderedDict()\n",
    "        exp_updates={pi:exp_pi,a:exp_a,b:exp_b}\n",
    "        baum_welch_updates.update(updates)\n",
    "        baum_welch_updates.update(exp_updates)\n",
    "        \n",
    "        #Gradient descent:\n",
    "        \n",
    "        cost=-T.log(full_prob)\n",
    "        \n",
    "        pi_grad=T.grad(cost=cost,wrt=pi)\n",
    "        a_grad=T.grad(cost=cost,wrt=a)\n",
    "        b_grad=T.grad(cost=cost,wrt=b)\n",
    "        \n",
    "        lr=T.scalar()\n",
    "        \n",
    "        pi_upd=pi-lr*pi_grad\n",
    "        norm_pi_upd=pi_upd/pi_upd.sum()\n",
    "        \n",
    "        a_upd=a-lr*a_grad\n",
    "        norm_a_upd=a_upd/a_upd.sum(axis=1).T\n",
    "        \n",
    "        b_upd=b-lr*b_grad\n",
    "        norm_b_upd=b_upd/b_upd.sum(axis=0)\n",
    "        \n",
    "        gd_updates=OrderedDict()\n",
    "        grad_updates={pi:norm_pi_upd,\n",
    "                      a:norm_a_upd,\n",
    "                      b:norm_b_upd}\n",
    "        gd_updates.update(updates)\n",
    "        gd_updates.update(grad_updates)            \n",
    "        \n",
    "        #function definitions\n",
    "        \n",
    "        self.forward_fun = theano.function(inputs=[O], outputs=alpha, updates=updates)\n",
    "        \n",
    "        self.backward_fun = theano.function(inputs=[O], outputs=beta, updates=updates)\n",
    "        \n",
    "        self.full_prob_fun = theano.function(inputs=[O], outputs=full_prob, updates=updates)\n",
    "        \n",
    "        self.gamma_fun = theano.function(inputs=[O], outputs=gamma, updates=updates)\n",
    "        \n",
    "        self.viterbi_fun = theano.function(inputs=[O], outputs=[Q,vite_prob], updates=updates)\n",
    "    \n",
    "        self.xi_fun = theano.function(inputs=[O], outputs=xi, updates=updates)            \n",
    "        \n",
    "        self.baum_welch_fun = theano.function(inputs=[O], outputs=[full_prob,exp_mean_err], updates=baum_welch_updates)\n",
    "        \n",
    "        self.gd_fun = theano.function(inputs=[O,lr], outputs=cost, updates=gd_updates)\n",
    "    \n",
    "    def setModel(self,pi,a,b):\n",
    "        \n",
    "        self.pi.set_value(pi.astype(theano.config.floatX))\n",
    "        self.a.set_value(a.astype(theano.config.floatX))\n",
    "        self.b.set_value(b.astype(theano.config.floatX))\n",
    "    \n",
    "    def forward(self, O):\n",
    "        \n",
    "        return self.forward_fun(O.astype('int32')) \n",
    "    \n",
    "    \n",
    "    def backward(self, O):\n",
    "        \n",
    "        return self.backward_fun(O.astype('int32'))        \n",
    "    \n",
    "    def full_prob(self, O):\n",
    "        \n",
    "        return self.full_prob_fun(O.astype('int32'))\n",
    "    \n",
    "    def gamma(self, O):\n",
    "        \n",
    "        return self.gamma_fun(O.astype('int32'))\n",
    "    \n",
    "    def viterbi(self, O):\n",
    "        \n",
    "        return self.viterbi_fun(O.astype('int32'))\n",
    "    \n",
    "    def xi(self, O):\n",
    "        \n",
    "        return self.xi_fun(O.astype('int32'))\n",
    "    \n",
    "    def baum_welch(self,O):\n",
    "        \n",
    "        return self.baum_welch_fun(O.astype('int32'))\n",
    "    \n",
    "    def gradient_descent(self,O,lr=0.01):\n",
    "        \n",
    "        return self.gd_fun(O.astype('int32'),lr)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation\n",
    "\n",
    "We can either use the default (all equally probable) or some other random values to begin with. Here we will read the model parameters from a file created in my other notebook. It will allow us to make sure all the calculations match the ones there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 3\n",
      "Number of observation classes: 4\n",
      "Number of time steps: 5\n",
      "Observation sequence: [3 1 1 2 0]\n",
      "Priors: [ 0.78380037  0.07988522  0.13631441]\n",
      "Transition matrix:\n",
      "[[ 0.19762551  0.14294511  0.65942938]\n",
      " [ 0.35323748  0.24211448  0.40464804]\n",
      " [ 0.12763736  0.41033146  0.46203117]]\n",
      "Observation probability matrix:\n",
      "[[ 0.37536286  0.21435122  0.32037336  0.33995268]\n",
      " [ 0.28848115  0.35628436  0.14879559  0.65702006]\n",
      " [ 0.33615598  0.42936442  0.53083105  0.00302726]]\n"
     ]
    }
   ],
   "source": [
    "with open('../data/hmm.pkl') as f:\n",
    "    O,pi,a,b,N,M,Time=pickle.load(f)\n",
    "    \n",
    "print 'Number of states: {}'.format(N)\n",
    "print 'Number of observation classes: {}'.format(M)\n",
    "print 'Number of time steps: {}'.format(Time) #T is taken by theano.tensor\n",
    "print 'Observation sequence: {}'.format(O)\n",
    "print 'Priors: {}'.format(pi)\n",
    "print 'Transition matrix:\\n{}'.format(a)\n",
    "print 'Observation probability matrix:\\n{}'.format(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will contruct the HMM object. The constructor needs to compile everything and since we have a few functions, it may take a little while:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.7 s, sys: 172 ms, total: 17.8 s\n",
      "Wall time: 17.8 s\n"
     ]
    }
   ],
   "source": [
    "%time hmm=DiscreteHMM(N=N,M=M)\n",
    "\n",
    "#we can also set the model parameters\n",
    "hmm.setModel(pi,a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms\n",
    "\n",
    "Let's test the methots now. You can compare the values with the ones from my other notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward probabilities:\n",
      "[[ 0.26645502  0.05248619  0.00041266]\n",
      " [ 0.01527275  0.01815819  0.08464377]\n",
      " [ 0.00433764  0.01471865  0.0242707 ]\n",
      " [ 0.00293278  0.00210437  0.01063258]\n",
      " [ 0.00100599  0.00152653  0.00258775]]\n",
      "Backward probabilities:\n",
      "[[ 0.01634315  0.0144613   0.01584348]\n",
      " [ 0.04414593  0.04067532  0.04380064]\n",
      " [ 0.14111529  0.1194201   0.11332435]\n",
      " [ 0.33708936  0.33846256  0.32159775]\n",
      " [ 1.          1.          1.        ]]\n",
      "Full model probability: 0.00512027181685\n",
      "Complete state probability:\n",
      "[[ 0.85048521  0.14823793  0.00127688]\n",
      " [ 0.13167854  0.14424822  0.72407317]\n",
      " [ 0.11954594  0.34328309  0.53717095]\n",
      " [ 0.1930774   0.13910387  0.66781867]\n",
      " [ 0.19647208  0.29813427  0.50539362]]\n",
      "Viterbi sequence: [0 2 2 2 0] its probability 0.000175862107426\n",
      "State transition probability:\n",
      "[[[  9.73174050e-02   1.07802279e-01   6.45365417e-01]\n",
      "  [  3.42637934e-02   3.59666944e-02   7.80074447e-02]\n",
      "  [  9.73402493e-05   4.79248323e-04   7.00286706e-04]]\n",
      "\n",
      " [[  1.78306568e-02   1.81412734e-02   9.57066044e-02]\n",
      "  [  3.78918946e-02   3.65320854e-02   6.98242411e-02]\n",
      "  [  6.38233870e-02   2.88609743e-01   3.71640086e-01]]\n",
      "\n",
      " [[  1.80802811e-02   6.09860849e-03   9.53670442e-02]\n",
      "  [  1.09658718e-01   3.50506753e-02   1.98573694e-01]\n",
      "  [  6.53384030e-02   9.79545787e-02   3.73877943e-01]]\n",
      "\n",
      " [[  4.24894094e-02   2.36196332e-02   1.26968354e-01]\n",
      "  [  5.44937514e-02   2.87056100e-02   5.59045114e-02]\n",
      "  [  9.94889140e-02   2.45809019e-01   3.22520792e-01]]]\n"
     ]
    }
   ],
   "source": [
    "print 'Forward probabilities:\\n{}'.format(hmm.forward(O))\n",
    "print 'Backward probabilities:\\n{}'.format(hmm.backward(O))\n",
    "print 'Full model probability: {}'.format(hmm.full_prob(O))\n",
    "print 'Complete state probability:\\n{}'.format(hmm.gamma(O))\n",
    "seq,vite_prob=hmm.viterbi(O)\n",
    "print 'Viterbi sequence: {} its probability {}'.format(seq,vite_prob)\n",
    "print 'State transition probability:\\n{}'.format(hmm.xi(O))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baum-Welch\n",
    "\n",
    "We will run 15 iterations of the Baum-Welch EM reestimation here. We will also output the model probability (which should increase with each iteration) and also the mean difference between the model parameters and their expected values (which will decrease to 0 as the model converges on the optimum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #1 P=0.00512027181685 delta_exp=0.0247722174972\n",
      "Iteration #2 P=0.00472553214058 delta_exp=0.00643846765161\n",
      "Iteration #3 P=0.00895740184933 delta_exp=0.00366491661407\n",
      "Iteration #4 P=0.0141975516453 delta_exp=0.000873240816873\n",
      "Iteration #5 P=0.0171108879149 delta_exp=0.000643724633846\n",
      "Iteration #6 P=0.0183900222182 delta_exp=0.00185267126653\n",
      "Iteration #7 P=0.0206706672907 delta_exp=0.00560718541965\n",
      "Iteration #8 P=0.026864964515 delta_exp=0.00610663974658\n",
      "Iteration #9 P=0.038870152086 delta_exp=0.00184295291547\n",
      "Iteration #10 P=0.0508226118982 delta_exp=0.000590678479057\n",
      "Iteration #11 P=0.0574444383383 delta_exp=0.000154764056788\n",
      "Iteration #12 P=0.0601356551051 delta_exp=3.64723564417e-05\n",
      "Iteration #13 P=0.0613080821931 delta_exp=8.5957472038e-06\n",
      "Iteration #14 P=0.0618683956563 delta_exp=2.08958635994e-06\n",
      "Iteration #15 P=0.0621437616646 delta_exp=5.93567904161e-07\n",
      "Iteration #16 P=0.0622817054391 delta_exp=2.46383166314e-07\n",
      "Iteration #17 P=0.0623527467251 delta_exp=1.55892223574e-07\n",
      "Iteration #18 P=0.0623909272254 delta_exp=1.22084387044e-07\n",
      "Iteration #19 P=0.0624127276242 delta_exp=1.01798882213e-07\n",
      "Iteration #20 P=0.0624261945486 delta_exp=8.60829985072e-08\n",
      "Iteration #21 P=0.0624352768064 delta_exp=7.30966931428e-08\n",
      "Iteration #22 P=0.0624419227242 delta_exp=6.22612716938e-08\n",
      "Iteration #23 P=0.0624471567571 delta_exp=5.32407575804e-08\n",
      "Iteration #24 P=0.0624514594674 delta_exp=4.57346800431e-08\n",
      "Iteration #25 P=0.0624551773071 delta_exp=3.94724715136e-08\n",
      "Iteration #26 P=0.0624583959579 delta_exp=3.42301866851e-08\n",
      "Iteration #27 P=0.0624612495303 delta_exp=2.98155384826e-08\n",
      "Iteration #28 P=0.06246381253 delta_exp=2.60958845644e-08\n",
      "Iteration #29 P=0.0624661520123 delta_exp=2.29260397333e-08\n",
      "Iteration #30 P=0.0624682456255 delta_exp=2.02248937597e-08\n"
     ]
    }
   ],
   "source": [
    "hmm.setModel(pi,a,b)\n",
    "for i in range(30):    \n",
    "    prob,exp_err=hmm.baum_welch(O)\n",
    "    print 'Iteration #{} P={} delta_exp={}'.format(i+1,prob,exp_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Since this is Theano, we can easily implement GD using the built-in *grad* method. The cost is set as the negative log of the complete model probability (the sum of the last step of forward probabilities). The parameters are updated by subtracting the gradient of the negative log likelihood multiplied by a learning rate. The updated values have to also be renormalized to keep the stochasticity of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #1 P=0.00512027274817\n",
      "Iteration #2 P=0.00613689888269\n",
      "Iteration #3 P=0.00783415138721\n",
      "Iteration #4 P=0.00776180718094\n",
      "Iteration #5 P=0.00900354515761\n",
      "Iteration #6 P=0.00957148615271\n",
      "Iteration #7 P=0.0107465116307\n",
      "Iteration #8 P=0.0114053161815\n",
      "Iteration #9 P=0.0121842995286\n",
      "Iteration #10 P=0.0125104524195\n",
      "Iteration #11 P=0.012768981047\n",
      "Iteration #12 P=0.012803777121\n",
      "Iteration #13 P=0.0129273673519\n",
      "Iteration #14 P=0.0135532636195\n",
      "Iteration #15 P=0.0156619008631\n",
      "Iteration #16 P=0.0211016517133\n",
      "Iteration #17 P=0.0307422261685\n",
      "Iteration #18 P=0.0418569408357\n",
      "Iteration #19 P=0.0502914786339\n",
      "Iteration #20 P=0.0551136508584\n",
      "Iteration #21 P=0.0574619658291\n",
      "Iteration #22 P=0.0588321425021\n",
      "Iteration #23 P=0.0598732866347\n",
      "Iteration #24 P=0.0607092119753\n",
      "Iteration #25 P=0.0613107830286\n",
      "Iteration #26 P=0.0616954192519\n",
      "Iteration #27 P=0.0619234070182\n",
      "Iteration #28 P=0.0620619431138\n",
      "Iteration #29 P=0.0621535144746\n",
      "Iteration #30 P=0.0622175484896\n",
      "Iteration #31 P=0.0622608475387\n",
      "Iteration #32 P=0.0622878856957\n",
      "Iteration #33 P=0.0623032264411\n",
      "Iteration #34 P=0.0623114705086\n",
      "Iteration #35 P=0.0623158700764\n",
      "Iteration #36 P=0.0623183511198\n",
      "Iteration #37 P=0.0623197928071\n",
      "Iteration #38 P=0.0623206086457\n",
      "Iteration #39 P=0.0623209960759\n",
      "Iteration #40 P=0.0623211301863\n",
      "Iteration #41 P=0.0623211152852\n",
      "Iteration #42 P=0.0623210854828\n",
      "Iteration #43 P=0.0623210407794\n",
      "Iteration #44 P=0.062321010977\n",
      "Iteration #45 P=0.062321010977\n",
      "Iteration #46 P=0.0623209662735\n",
      "Iteration #47 P=0.0623209513724\n",
      "Iteration #48 P=0.0623208768666\n",
      "Iteration #49 P=0.0623208917677\n",
      "Iteration #50 P=0.0623208768666\n"
     ]
    }
   ],
   "source": [
    "hmm.setModel(pi,a,b)\n",
    "for i in range(50):    \n",
    "    prob=hmm.gradient_descent(O,1)\n",
    "    print 'Iteration #{} P={}'.format(i+1,np.exp(-prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing this with the Baum-Welch should make it clear it's not that much better to do it this way. According to the Rabiner paper, they should be roughly equivalent, although the renormalization is implemented slightly differently there. \n",
    "\n",
    "Tuning the learning rate may help the convergence and one may also think about other techniques (momentum, dynamic learning rate, Newton's method, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log model\n",
    "\n",
    "*work-in-progess*\n",
    "\n",
    "Log arithmetic hints: [here](https://github.com/UFAL-DSG/alex/blob/master/alex/ml/logarithmetic.py)\n",
    "\n",
    "This is a copy of the code above, but keeping everythin in the log domain. We assume all the values (parameters, etc) are logarithms of what they usually are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pylearn2.expr.basic import log_sum_exp\n",
    "\n",
    "def LogDot(a,b):\n",
    "    return log_sum_exp(a + b)\n",
    "\n",
    "def LogSum(a,axis=None):\n",
    "    if axis is None:        \n",
    "        a_max = a.max()\n",
    "        return a_max + T.log((T.exp(a - a_max)).sum())\n",
    "    \n",
    "    shp = a.shape\n",
    "    shp[axis] = 1\n",
    "    a_max = a.max(axis=axis)\n",
    "    s = T.log(T.exp(a - a_max.reshape(shp)).sum(axis=axis))\n",
    "    lse = a_max + s\n",
    "    return lse\n",
    "\n",
    "def LogAdd(a,b):\n",
    "    return T.log(T.exp(a)+T.exp(b))\n",
    "\n",
    "def LogSub(a,b):\n",
    "    return T.log(T.exp(a)-T.exp(b))\n",
    "\n",
    "def LogMean(a):\n",
    "    return LogSum(a)-T.log(a.shape[0])\n",
    "\n",
    "class LogDiscreteHMM:\n",
    "    \n",
    "    def __init__(self, N=3, M=4):        \n",
    "        \n",
    "        updates={}\n",
    "                \n",
    "        pi = theano.shared((np.zeros(N)/N).astype(theano.config.floatX))\n",
    "        a = theano.shared((np.zeros((N,N))/(N*np.ones(N))).astype(theano.config.floatX))\n",
    "        b = theano.shared((np.zeros((N,M))/(N*np.ones(M))).astype(theano.config.floatX))\n",
    "        \n",
    "        self.pi=pi\n",
    "        self.a=a\n",
    "        self.b=b\n",
    "        \n",
    "        O = T.ivector()\n",
    "        TT = O.shape[0]\n",
    "        \n",
    "        \n",
    "        #forward algorithm:\n",
    "        \n",
    "        alpha0=pi+b[:,O[0]]\n",
    "        \n",
    "        alpha_scan,upd = theano.scan(fn=lambda O,alpha_p: LogDot(alpha_p,a)+b[:,O],\n",
    "                               sequences=O[1:],\n",
    "                               outputs_info=alpha0)\n",
    "        \n",
    "        updates.update(upd)\n",
    "        \n",
    "        alpha=T.concatenate((alpha0.reshape((1,N)),alpha_scan))                \n",
    "        \n",
    "        #backward algorithm:\n",
    "        \n",
    "        beta0=T.zeros(N).astype(theano.config.floatX)\n",
    "        \n",
    "        beta_scan,upd = theano.scan(fn=lambda O,beta_p: LogDot(beta_p+b[:,O],a.T),\n",
    "                               sequences=O[1:],\n",
    "                               outputs_info=beta0,\n",
    "                               go_backwards=True)\n",
    "        updates.update(upd)\n",
    "        \n",
    "        beta=T.concatenate((beta_scan[::-1],beta0.reshape((1,N))))        \n",
    "        \n",
    "        #full model probability:\n",
    "        \n",
    "        full_prob = LogSum(alpha_scan[-1])\n",
    "        \n",
    "        #forward-backward probabilities:\n",
    "        \n",
    "        gamma=alpha+beta-full_prob        \n",
    "        \n",
    "        #viterbi algorithm:\n",
    "        \n",
    "        def viterbi_rec_step(O, delta_p, phi_p):\n",
    "            m=delta_p+a.T\n",
    "            phi=m.argmax(axis=1)\n",
    "            delta=m[T.arange(N),phi]+b[:,O]\n",
    "            return delta,phi\n",
    "        \n",
    "        phi0=T.zeros(N).astype('int64')\n",
    "\n",
    "        [delta_scan, phi_scan], upd = theano.scan(fn=viterbi_rec_step,\n",
    "                                                  sequences=O[1:],\n",
    "                                                  outputs_info=[alpha0,phi0])        \n",
    "        \n",
    "        updates.update(upd)\n",
    "        \n",
    "        QT=phi_scan[-1].argmax()        \n",
    "        vite_prob = delta_scan[-1,QT]\n",
    "        \n",
    "        Q_scan, upd = theano.scan(fn=lambda phi, Q: phi[Q],\n",
    "                             sequences=phi_scan,\n",
    "                             outputs_info=QT,\n",
    "                             go_backwards=True)\n",
    "        \n",
    "        updates.update(upd)\n",
    "                                                  \n",
    "        Q=T.concatenate((Q_scan[::-1],QT.reshape((1,))))\n",
    "        \n",
    "        #transition probabilities\n",
    "        \n",
    "        xi=alpha[:-1].reshape((TT-1,N,1))+a.reshape((1,N,N))+b[:,O[1:]].T.reshape((TT-1,1,N))+beta[1:].reshape((TT-1,1,N))-full_prob\n",
    "        \n",
    "        #expected values\n",
    "        \n",
    "        exp_pi=gamma[0]\n",
    "        \n",
    "        exp_a=LogSum(xi,axis=0)-LogSum(gamma[:-1],axis=0).reshape((N,1))\n",
    "        \n",
    "        exp_b_map, upd = theano.map(fn=lambda k: LogSum(gamma[T.eq(O,k).nonzero()],axis=0)-LogSum(gamma,axis=0), \n",
    "                         sequences=T.arange(M))\n",
    "        \n",
    "        updates.update(upd)\n",
    "        \n",
    "        exp_b = exp_b_map.T\n",
    "        \n",
    "        exp_err = T.concatenate((LogSub(pi,exp_pi).ravel(),LogSub(a,exp_a).ravel(),LogSub(b,exp_b).ravel()))\n",
    "        \n",
    "        exp_mean_err = LogMean(exp_err+2)\n",
    "        \n",
    "        #Baum-Welch updates:\n",
    "        \n",
    "        baum_welch_updates=OrderedDict()\n",
    "        exp_updates={pi:exp_pi,a:exp_a,b:exp_b}\n",
    "        baum_welch_updates.update(updates)\n",
    "        baum_welch_updates.update(exp_updates)\n",
    "        \n",
    "        #Gradient descent:\n",
    "        \n",
    "        cost=-T.log(full_prob)\n",
    "        \n",
    "        pi_grad=T.grad(cost=cost,wrt=pi)\n",
    "        a_grad=T.grad(cost=cost,wrt=a)\n",
    "        b_grad=T.grad(cost=cost,wrt=b)\n",
    "        \n",
    "        lr=T.scalar()\n",
    "        \n",
    "        pi_upd=LogSub(pi,lr+pi_grad)\n",
    "        norm_pi_upd=pi_upd-LogSum(pi_upd)\n",
    "        \n",
    "        a_upd=LogSub(a,lr+a_grad)\n",
    "        norm_a_upd=a_upd-LogSum(a_upd,axis=1).T\n",
    "        \n",
    "        b_upd=LogSub(b,lr+b_grad)\n",
    "        norm_b_upd=b_upd-LogSum(b_upd,axis=0)\n",
    "        \n",
    "        gd_updates=OrderedDict()\n",
    "        grad_updates={pi:norm_pi_upd,\n",
    "                      a:norm_a_upd,\n",
    "                      b:norm_b_upd}\n",
    "        gd_updates.update(updates)\n",
    "        gd_updates.update(grad_updates)            \n",
    "        \n",
    "        #function definitions\n",
    "        \n",
    "        self.forward_fun = theano.function(inputs=[O], outputs=alpha, updates=updates)\n",
    "        \n",
    "        self.backward_fun = theano.function(inputs=[O], outputs=beta, updates=updates)\n",
    "        \n",
    "        self.full_prob_fun = theano.function(inputs=[O], outputs=full_prob, updates=updates)\n",
    "        \n",
    "        self.gamma_fun = theano.function(inputs=[O], outputs=gamma, updates=updates)\n",
    "        \n",
    "        self.viterbi_fun = theano.function(inputs=[O], outputs=[Q,vite_prob], updates=updates)\n",
    "    \n",
    "        self.xi_fun = theano.function(inputs=[O], outputs=xi, updates=updates)            \n",
    "        \n",
    "        self.baum_welch_fun = theano.function(inputs=[O], outputs=[full_prob,exp_mean_err], updates=baum_welch_updates)\n",
    "        \n",
    "        self.gd_fun = theano.function(inputs=[O,lr], outputs=cost, updates=gd_updates)\n",
    "    \n",
    "    def setModel(self,pi,a,b):\n",
    "        \n",
    "        self.pi.set_value(pi.astype(theano.config.floatX))\n",
    "        self.a.set_value(a.astype(theano.config.floatX))\n",
    "        self.b.set_value(b.astype(theano.config.floatX))\n",
    "    \n",
    "    def forward(self, O):\n",
    "        \n",
    "        return self.forward_fun(O.astype('int32')) \n",
    "    \n",
    "    \n",
    "    def backward(self, O):\n",
    "        \n",
    "        return self.backward_fun(O.astype('int32'))        \n",
    "    \n",
    "    def full_prob(self, O):\n",
    "        \n",
    "        return self.full_prob_fun(O.astype('int32'))\n",
    "    \n",
    "    def gamma(self, O):\n",
    "        \n",
    "        return self.gamma_fun(O.astype('int32'))\n",
    "    \n",
    "    def viterbi(self, O):\n",
    "        \n",
    "        return self.viterbi_fun(O.astype('int32'))\n",
    "    \n",
    "    def xi(self, O):\n",
    "        \n",
    "        return self.xi_fun(O.astype('int32'))\n",
    "    \n",
    "    def baum_welch(self,O):\n",
    "        \n",
    "        return self.baum_welch_fun(O.astype('int32'))\n",
    "    \n",
    "    def gradient_descent(self,O,lr=0.01):\n",
    "        \n",
    "        return self.gd_fun(O.astype('int32'),lr)        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "When compiling the inner function of scan (the function called by scan in each of its iterations) the following error has been encountered: The initial state (`outputs_info` in scan nomenclature) of variable IncSubtensor{Set;:int64:}.0 (argument number 1) has 2 dimension(s), while the corresponding variable in the result of the inner function of scan (`fn`) has 0 dimension(s) (it should be one less than the initial state). For example, if the inner function of scan returns a vector of size d and scan uses the values of the previous time-step, then the initial state in scan should be a matrix of shape (1, d). The first dimension of this matrix corresponds to the number of previous time-steps that scan uses in each of its iterations. In order to solve this issue if the two varialbe currently have the same dimensionality, you can increase the dimensionality of the variable in the initial state of scan by using dimshuffle or shape_padleft. ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-1d3593dc0ffd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloghmm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLogDiscreteHMM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#we can also set the model parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mloghmm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-42db5a7e04dd>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, N, M)\u001b[0m\n\u001b[0;32m     62\u001b[0m                                \u001b[0msequences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mO\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m                                \u001b[0moutputs_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbeta0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m                                go_backwards=True)\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[0mupdates\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/guest/Theano/theano/scan_module/scan.pyc\u001b[0m in \u001b[0;36mscan\u001b[1;34m(fn, sequences, outputs_info, non_sequences, n_steps, truncate_gradient, go_backwards, mode, name, profile, allow_gc, strict)\u001b[0m\n\u001b[0;32m   1039\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1040\u001b[0m         \u001b[0mscan_inputs\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1041\u001b[1;33m     \u001b[0mscan_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlocal_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mscan_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1042\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscan_outs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m         \u001b[0mscan_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mscan_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/guest/Theano/theano/gof/op.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    609\u001b[0m         \"\"\"\n\u001b[0;32m    610\u001b[0m         \u001b[0mreturn_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'return_list'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m         \u001b[0mnode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_test_value\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'off'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/guest/Theano/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mmake_node\u001b[1;34m(self, *inputs)\u001b[0m\n\u001b[0;32m    554\u001b[0m                                   \u001b[0margoffset\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m                                   \u001b[0mouter_sitsot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 556\u001b[1;33m                                   inner_sitsot_out.type.ndim))\n\u001b[0m\u001b[0;32m    557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[0margoffset\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mouter_sitsot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: When compiling the inner function of scan (the function called by scan in each of its iterations) the following error has been encountered: The initial state (`outputs_info` in scan nomenclature) of variable IncSubtensor{Set;:int64:}.0 (argument number 1) has 2 dimension(s), while the corresponding variable in the result of the inner function of scan (`fn`) has 0 dimension(s) (it should be one less than the initial state). For example, if the inner function of scan returns a vector of size d and scan uses the values of the previous time-step, then the initial state in scan should be a matrix of shape (1, d). The first dimension of this matrix corresponds to the number of previous time-steps that scan uses in each of its iterations. In order to solve this issue if the two varialbe currently have the same dimensionality, you can increase the dimensionality of the variable in the initial state of scan by using dimshuffle or shape_padleft. "
     ]
    }
   ],
   "source": [
    "loghmm=LogDiscreteHMM(N=N,M=M)\n",
    "\n",
    "#we can also set the model parameters\n",
    "loghmm.setModel(pi,a,b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
