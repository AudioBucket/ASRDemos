{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple MLP demo for TIMIT using Keras\n",
    "\n",
    "This notebook describes how to reproduce the results for the simple MLP architecture described in this paper:\n",
    "\n",
    "[ftp://ftp.idsia.ch/pub/juergen/nn_2005.pdf](ftp://ftp.idsia.ch/pub/juergen/nn_2005.pdf)\n",
    "\n",
    "And in Chapter 5 of this thesis:\n",
    "\n",
    "http://www.cs.toronto.edu/~graves/phd.pdf\n",
    "\n",
    "To begin with, if you have a multi-gpu system (like I do), you may want to choose which GPU you want to run this on (indexing from 0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import the stuff we use below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K80 (CNMeM is enabled with initial size: 90.0% of memory, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from tqdm import *\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('../python')\n",
    "\n",
    "from data import Corpus, History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "Here we load the corpus stored in HDF5 files. It contains both normalized and unnormalized data and we're interested in the former:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train=Corpus('../data/TIMIT_train.hdf5',load_normalized=True)\n",
    "dev=Corpus('../data/TIMIT_dev.hdf5',load_normalized=True)\n",
    "test=Corpus('../data/TIMIT_test.hdf5',load_normalized=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data can be loaded all at once into separate Numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_in,tr_out_dec=train.get()\n",
    "dev_in,dev_out_dec=dev.get()\n",
    "tst_in,tst_out_dec=test.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaded data is a list of utterances, where each utterance is a matrix (for inputs) or a vector (for outputs) of different sizes. That is why the whole corpus is not a matrix (which would require that each utterance is the same length):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3696,)\n",
      "(159, 39)\n",
      "(3696,)\n",
      "(159,)\n"
     ]
    }
   ],
   "source": [
    "print tr_in.shape\n",
    "print tr_in[0].shape\n",
    "print tr_out_dec.shape\n",
    "print tr_out_dec[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The papers/thesis above use 26 features instead of the standrd 39, ie they only use first-order regression coefficients (deltas). We usually prepare a corpus for the full 39 features, so to be comparable, lets extract the 26 from that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for u in range(tr_in.shape[0]):\n",
    "    tr_in[u]=tr_in[u][:,:26]\n",
    "for u in range(dev_in.shape[0]):    \n",
    "    dev_in[u]=dev_in[u][:,:26]\n",
    "for u in range(tst_in.shape[0]):\n",
    "    tst_in[u]=tst_in[u][:,:26]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Here we'll define some standard sizes and parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_dim=tr_in[0].shape[1]\n",
    "output_dim=61\n",
    "hidden_num=250\n",
    "epoch_num=1500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-hot vectors\n",
    "\n",
    "For most loss functions, the output for each utterance needs to be a matrix of size (output_dim,sample_num). That means we need to convert the output from a list of decisions to a list of 1-hot vectors. This is a requirement of Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dec2onehot(dec):\n",
    "    ret=[]\n",
    "    for u in dec:\n",
    "        assert np.all(u<output_dim)\n",
    "        num=u.shape[0]\n",
    "        r=np.zeros((num,output_dim))\n",
    "        r[range(0,num),u]=1\n",
    "        ret.append(r)\n",
    "    return np.array(ret)\n",
    "\n",
    "tr_out=dec2onehot(tr_out_dec)\n",
    "dev_out=dec2onehot(dev_out_dec)\n",
    "tst_out=dec2onehot(tst_out_dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition\n",
    "\n",
    "Here we define the model exactly as in the paper: one hidden layer with 250 units, sigmoid activation in the hidden and softmax in the output, cross-entropy loss. The only thing that differs is the optimizer. You can use SGD, but the values in the paper seem to be far too small. Adam works just as well and maybe even a bit faster. Feel free to experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(input_dim=input_dim,output_dim=hidden_num))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(output_dim=output_dim))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#optimizer= Adam()\n",
    "optimizer= SGD(lr=3e-3,momentum=0.9,nesterov=False)\n",
    "loss='categorical_crossentropy'\n",
    "metrics=['accuracy']\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Here we have a training loop. We don't use the \"fit\" method to accomodate the specific conditions in the paper: we register the loss/accuracy of dev and test at each time step, we do weight update after each utterance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 2.01809406281\n",
      "Train PER: 58.660785%\n",
      "Dev loss: 2.01716089249\n",
      "Dev PER: 58.178419%\n",
      "Test loss: 2.02342534065\n",
      "Test PER: 58.169167%\n",
      "Epoch #25/1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 1526/3696 [00:01<00:02, 887.34it/s]"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "tr_hist=History('Train')\n",
    "dev_hist=History('Dev')\n",
    "tst_hist=History('Test')\n",
    "\n",
    "tr_it=range(tr_in.shape[0])\n",
    "\n",
    "for e in range(epoch_num):\n",
    "    \n",
    "    print 'Epoch #{}/{}'.format(e+1,epoch_num)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    shuffle(tr_it)\n",
    "    for u in tqdm(tr_it):\n",
    "        l,a=model.train_on_batch(tr_in[u],tr_out[u])\n",
    "        tr_hist.r.addLA(l,a,tr_out[u].shape[0])\n",
    "    clear_output()    \n",
    "    tr_hist.log()\n",
    "    \n",
    "    for u in range(dev_in.shape[0]):\n",
    "        l,a=model.test_on_batch(dev_in[u],dev_out[u])\n",
    "        dev_hist.r.addLA(l,a,dev_out[u].shape[0])\n",
    "    dev_hist.log()\n",
    "    \n",
    "    \n",
    "    for u in range(tst_in.shape[0]):\n",
    "        l,a=model.test_on_batch(tst_in[u],tst_out[u])\n",
    "        tst_hist.r.addLA(l,a,tst_out[u].shape[0])\n",
    "    tst_hist.log()            \n",
    "    \n",
    "print 'Done!'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Here we can plot the loss and PER (phoneme error rate) while training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as P\n",
    "%matplotlib inline\n",
    "\n",
    "fig,ax=P.subplots(2,sharex=True,figsize=(12,10))\n",
    "\n",
    "ax[0].set_title('Loss')\n",
    "ax[0].plot(tr_hist.loss,label='Train')\n",
    "ax[0].plot(dev_hist.loss,label='Dev')\n",
    "ax[0].plot(tst_hist.loss,label='Test')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].set_title('PER %')\n",
    "ax[1].plot(100*(1-np.array(tr_hist.acc)),label='Train')\n",
    "ax[1].plot(100*(1-np.array(dev_hist.acc)),label='Dev')\n",
    "ax[1].plot(100*(1-np.array(tst_hist.acc)),label='Test')\n",
    "ax[1].legend()\n",
    "#ax[1].set_ylim((49,51))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final results are presented below. Please note that Keras usually calculates accuracy, while the papers generally prefer error rates. We generally shouldn't give the result of the minimum PER for the test set, but we can use the dev set, find it's minimum and provide the value of the test at that time. You can see that the correct PER is not too far from the minimum test PER anyway:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Min train PER: {:%}'.format(1-np.max(tr_hist.acc))\n",
    "print 'Min test PER: {:%}'.format(1-np.max(tst_hist.acc))\n",
    "print 'Min dev PER epoch: #{}'.format((np.argmax(dev_hist.acc)+1))\n",
    "print 'Test PER on min dev: {:%}'.format(1-tst_hist.acc[np.argmax(dev_hist.acc)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper gives a value of 48.6% error rate for this architecture and claims it took 835 epochs to reach the value using SGD. Here we can see that ADAM got it a bit faster than that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wer=0.486999999999\n",
    "print 'Epoch where PER reached {:%}: #{}'.format(wer,np.where((1-np.array(tst_hist.acc))<=wer)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
