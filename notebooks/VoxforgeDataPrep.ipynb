{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Voxforge database\n",
    "\n",
    "This notebook will demonstrate how to prepare the free [Voxforge](http://www.voxforge.org/) database for training. This database is a small-to-medium sized database available online for free under the GPL license. A much more common database used in most research is the [TIMIT](https://catalog.ldc.upenn.edu/LDC93S1), but that costs $250 and also isn't too large (although much more professionally developed than Voxforge). The best alternative today is the [Librispeech](http://www.openslr.org/12/) database, but that has a few dozen GB of data and wouldn't be sensible for a simple demo. So Voxforge it is...\n",
    "\n",
    "First thing to do is realize what a speech corpus actually is: in its simplest form it is a collection of audio files (containing preferably speech only) with a set of transcripts of the speech. There are a few extensions to this that are worth noting:\n",
    "  * phonemes - transcripts are usually presented as a list of words - although not a rule, it is often easier to start the recognition process with phonemes and go from there. Voxforge defines a list of 39 phonemes (+ silence) and contains a lexicon mapping the words into phonemes (more about that below)\n",
    "  * aligned speech - the transcripts are usually just a sequence of words/phonemes, but they don't denote which word/phoneme occurs when - there are models that can learn from that (seq2seq learning), but having alignments is usually a big plus. TIMIT was hand-aligned by a group of professionals (which is why its a popular resource for research), but Voxforge wasn't. Fortunately, we can use one of the many available tools to do this automatically (with a margin of error - more on that below)\n",
    "  * meta-data - each recording session in the Voxforge database contains a readme file with useful information about the speaker and the environment that the recording took place in. When making a serious speech recognizer, this information can be very useful (e.g. for speaker adaptation - taking into account the speaker id, gender, age, etc...)\n",
    "  \n",
    "## Downloading the corpus\n",
    "\n",
    "To start working with the corpus, it needs to be downloaded first. All the files can be found in the download section of the Voxforge website under this URL:\n",
    "\n",
    "http://www.repository.voxforge1.org/downloads/SpeechCorpus/Trunk/Audio/Main/16kHz_16bit/\n",
    "\n",
    "There are 2 versions of the main corpus: sampled at 16kHz and 8kHz. The 16 kHz one is of better quality and is known as \"desktop quality speech\". While the original recordings were made at an even higher quality (44.1 kHz), 16k is completely sufficient for recoginzing speech (higher quality doesn't help much). 8 kHz is known as the telephony quality and is a standard value for the old (uncompressed, aka T0) digital telephone signal. If you are making a recognizer that has to work in the telephony environment, you should use this data instread\n",
    "\n",
    "To download the whole dataset, a small program in Python is included in this demo. Be warned, this can take a long time (I think Voxforge is throttling the speed to save on costs) and restarts may be neccessary. The python method does check for failed downloads (compares file sizes) and restarts whatever wasn't downloaded completely, so you can run the method 2-3 times to make sure everything is ok.\n",
    "\n",
    "Alternatively, wou can use a program like wget and enter this command (where \"audio\" is the dir to save the data to):\n",
    "\n",
    "    wget -P audio -l 1 -N -nd -c -e robots=off -A tgz -r -np http://www.repository.voxforge1.org/downloads/SpeechCorpus/Trunk/Audio/Main/16kHz_16bit\n",
    " \n",
    "First lets import all the voxforge methods from the python directory. These will need the following libraries installed on your system:\n",
    "  * numpy - for working with data\n",
    "  * random, urllib, lxml, os, tarfile, gzip, re, pickle, shutil - these are standard system libraries and anyone should have them\n",
    "  * scikits.audiolab - to load the audio files from the database (WAV and FLAC files)\n",
    "  * tqdm - a [simple library for progressbars](https://github.com/noamraph/tqdm) that you can install using pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/scikits/audiolab/soundio/play.py:48: UserWarning: Could not import alsa backend; most probably, you did not have alsa headers when building audiolab\n",
      "  warnings.warn(\"Could not import alsa backend; most probably, \"\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../python')\n",
    "\n",
    "from voxforge import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore any warnings above (I coudn't be bothered to compile audiolab with Alsa). Below you will find the method to download the Voxforge database. You only need to do this once, so you can run it either here or from a console or use wget. Be warned that it takes a long time (as mentioned earlier) so it's a good idea to leave it running over night. I already did it once, so I'll skip it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "downloadVoxforgeData('../audio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the corpus\n",
    "\n",
    "Once the data is downloaded and stored in the 'audio' subdir of the main project dir, we can start loading the data into a Python datastructure. There are several methods that can be used for that. The following method will load a file and display its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Pronunciation dialect': 'American English', 'File type': 'wav', 'Age Range': 'Adult', 'Speaker Characteristics': '', 'Language': 'EN', 'File Info': '', 'Gender': 'Male', 'Audio Recording Software': 'VoxForge Speech Submission Application', 'Audio card type': 'unknown', 'User Name': 'Joel', 'Sample rate format': '16', 'Number of channels': '1', 'O/S': '', 'Microphone make': 'n/a', 'Path': 'Joel-20080716-qoz', 'Sampling Rate': '48000', 'Microphone type': 'USB Headset mic', 'Recording Information': '', 'Audio card make': 'unknown'}\n",
      "{'b0076': ['BEFORE', 'PHILIP', 'COULD', 'RECOVER', 'HIMSELF', \"JEANNE'S\", 'STARTLED', 'GUARDS', 'WERE', 'UPON', 'HIM'], 'b0077': ['IT', 'IS', 'THE', 'NEAREST', 'REFUGE'], 'b0074': ['YET', 'BEHIND', 'THEM', 'THERE', 'WAS', 'ANOTHER', 'AND', 'MORE', 'POWERFUL', 'MOTIVE'], 'b0075': ['IN', 'THAT', 'CASE', 'HE', 'COULD', 'NOT', 'MISS', 'THEM', 'IF', 'HE', 'USED', 'CAUTION'], 'b0081': ['YOU', 'WERE', 'GOING', 'TO', 'LEAVE', 'AFTER', 'YOU', 'SAW', 'ME', 'ON', 'THE', 'ROCK'], 'b0080': ['TOMORROW', 'IT', 'WILL', 'BE', 'STRONG', 'ENOUGH', 'FOR', 'YOU', 'TO', 'STAND', 'UPON'], 'b0083': ['IN', 'IT', 'THERE', 'WAS', 'SOMETHING', 'THAT', 'WAS', 'ALMOST', 'TRAGEDY'], 'b0082': ['HE', 'BIT', 'HIS', 'TONGUE', 'AND', 'CURSED', 'HIMSELF', 'AT', 'THIS', 'FRESH', 'BREAK'], 'b0078': ['THERE', 'WAS', 'PRIDE', 'AND', 'STRENGTH', 'THE', 'RING', 'OF', 'TRIUMPH', 'IN', 'HIS', 'VOICE'], 'b0079': ['THE', 'TRUTH', 'OF', 'IT', 'SET', 'JEANNE', 'QUIVERING']}\n",
      "{'b0076': array([-1, -1, -1, ..., -1, -1, -2], dtype=int16), 'b0077': array([-1, -1, -1, ...,  0,  0, -1], dtype=int16), 'b0074': array([-1, -1, -2, ..., -1, -2, -2], dtype=int16), 'b0075': array([-1, -2, -1, ..., -2, -2, -1], dtype=int16), 'b0078': array([-1, -1, -1, ..., -1, -1, -2], dtype=int16), 'b0079': array([ 0, -1, -1, ..., -1, -1, -1], dtype=int16), 'b0083': array([-1, -2, -2, ..., -1, -2, -2], dtype=int16), 'b0082': array([-1, -1, -1, ..., -2, -2, -2], dtype=int16), 'b0081': array([-1, -1, -2, ..., -1, -2, -1], dtype=int16), 'b0080': array([-1,  0, -1, ..., -2, -1, -1], dtype=int16)}\n"
     ]
    }
   ],
   "source": [
    "f=loadFile('../audio/Joel-20080716-qoz.tgz')\n",
    "print f.props\n",
    "print f.prompts\n",
    "print f.data\n",
    "\n",
    "%xdel f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loadBySpeaker method will load the whole folder and organize its contents by speakers (as a dictionary). Each utterance contains only the data and the prompts. For this demo, only 30 files are read - as this isn't a method we are going to ultimately use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "corp=loadBySpeaker('../audio', limit=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus can also be extended by the phonetic transcription of the utterances using a lexicon file. Voxforge does provide such a file on its website and it is downloaded automatically (if it doesn't already exist).\n",
    "\n",
    "Note that a single word can have several transcriptions. In the lexicon, these alternatives will have sequential number suffixes added to the word (word, word2, word3, etc), but this particular function will do nothing about that. Choosing the right pronounciation variant has to be done either manually, or by using a more sophisticated program (a pre-trained ASR system) to choose the right version automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple_Eater', 'ryanjyoder', 'Perygryne', 'apdsqueaky', 'sharrington', 'yoyology', 'camdixon', 'Krellis', 'rocketman768', 'anonymous_9', 'anonymous_8', 'anonymous_5', 'anonymous_4', 'anonymous_7', 'anonymous_6', 'anonymous_1', 'anonymous_3', 'anonymous_2', 'ductapeguy', 'bhuvan', 'Primus', 'Q', 'adgar', 'thepinkcat', 'farmerjack', 'Steltek', 'TimS', 'pcsnpny']\n",
      "{'a0060': [array([ 278,  313,  139, ..., -443, -376, -179], dtype=int16), ['ANYWAY', 'NO', 'ONE', 'SAW', 'HER', 'LIKE', 'THAT'], ['eh', 'n', 'iy', 'w', 'ey', 'n', 'ow', 'w', 'ah', 'n', 's', 'ao', 'hh', 'er', 'l', 'ay', 'k', 'dh', 'ae', 't']], 'a0061': [array([-216, -258, -146, ..., -612, -567, -225], dtype=int16), ['PHILIP', 'SNATCHED', 'AT', 'THE', 'LETTER', 'WHICH', 'GREGSON', 'HELD', 'OUT', 'TO', 'HIM'], ['f', 'ih', 'l', 'ah', 'p', 's', 'n', 'ae', 'ch', 't', 'ae', 't', 'dh', 'ah', 'l', 'eh', 't', 'er', 'w', 'ih', 'ch', 'g', 'r', 'eh', 'g', 's', 'ah', 'n', 'hh', 'eh', 'l', 'd', 'aw', 't', 't', 'uw', 'hh', 'ih', 'm']], 'a0059': [array([ 566,  805,  615, ..., -226, -155,  -43], dtype=int16), ['HIS', 'IMMACULATE', 'APPEARANCE', 'WAS', 'GONE'], ['hh', 'ih', 'z', 'ih', 'm', 'ae', 'k', 'y', 'uw', 'l', 'ih', 't', 'ah', 'p', 'ih', 'r', 'ah', 'n', 's', 'w', 'aa', 'z', 'g', 'ao', 'n']], 'a0058': [array([ 99, 146, 251, ...,  21, 131,  64], dtype=int16), ['I', 'CAME', 'FOR', 'INFORMATION', 'MORE', 'OUT', 'OF', 'CURIOSITY', 'THAN', 'ANYTHING', 'ELSE'], ['ay', 'k', 'ey', 'm', 'f', 'ao', 'r', 'ih', 'n', 'f', 'ao', 'r', 'm', 'ey', 'sh', 'ah', 'n', 'm', 'ao', 'r', 'aw', 't', 'ah', 'v', 'k', 'y', 'uh', 'r', 'iy', 'aa', 's', 'ah', 't', 'iy', 'dh', 'ae', 'n', 'eh', 'n', 'iy', 'th', 'ih', 'ng', 'eh', 'l', 's']], 'a0057': [array([-432, -495, -253, ...,  259,  201,   32], dtype=int16), ['I', 'HAVE', 'NO', 'IDEA', 'REPLIED', 'PHILIP'], ['ay', 'hh', 'ae', 'v', 'n', 'ow', 'ay', 'd', 'iy', 'ah', 'r', 'ih', 'p', 'l', 'ay', 'd', 'f', 'ih', 'l', 'ah', 'p']], 'a0056': [array([-101,  -30,  113, ..., -406, -421, -314], dtype=int16), [\"PEARCE'S\", 'LITTLE', 'EYES', 'WERE', 'FIXED', 'ON', 'HIM', 'SHREWDLY'], ['p', 'ih', 'r', 's', 'ih', 'z', 'l', 'ih', 't', 'ah', 'l', 'ay', 'z', 'w', 'er', 'f', 'ih', 'k', 's', 't', 'aa', 'n', 'hh', 'ih', 'm', 'sh', 'r', 'uw', 'd', 'l', 'iy']], 'a0055': [array([ 953, 1215,  730, ...,   -3,  -62,  -60], dtype=int16), ['PHILIP', 'STOOD', 'UNDECIDED', 'HIS', 'EARS', 'STRAINED', 'TO', 'CATCH', 'THE', 'SLIGHTEST', 'SOUND'], ['f', 'ih', 'l', 'ah', 'p', 's', 't', 'uh', 'd', 'ah', 'n', 'd', 'ih', 's', 'ay', 'd', 'ih', 'd', 'hh', 'ih', 'z', 'ih', 'r', 'z', 's', 't', 'r', 'ey', 'n', 'd', 't', 'uw', 'k', 'ae', 'ch', 'dh', 'ah', 's', 'l', 'ay', 't', 'ah', 's', 't', 's', 'aw', 'n', 'd']], 'a0054': [array([304, 360, 264, ..., 395, 370, 143], dtype=int16), ['THERE', 'WAS', 'NOTHING', 'ON', 'THE', 'ROCK'], ['dh', 'eh', 'r', 'w', 'aa', 'z', 'n', 'ah', 'th', 'ih', 'ng', 'aa', 'n', 'dh', 'ah', 'r', 'aa', 'k']], 'a0053': [array([1209, 1442,  723, ...,  665,  726,  566], dtype=int16), ['SUDDENLY', 'HIS', 'FINGERS', 'CLOSED', 'TIGHTLY', 'OVER', 'THE', 'HANDKERCHIEF'], ['s', 'ah', 'd', 'ah', 'n', 'l', 'iy', 'hh', 'ih', 'z', 'f', 'ih', 'ng', 'g', 'er', 'z', 'k', 'l', 'ow', 'z', 'd', 't', 'ay', 't', 'l', 'iy', 'ow', 'v', 'er', 'dh', 'ah', 'hh', 'ae', 'ng', 'k', 'er', 'ch', 'ih', 'f']], 'a0052': [array([ -8,  78, 234, ..., 591, 420, 273], dtype=int16), ['IT', 'WAS', 'A', 'CURIOUS', 'COINCIDENCE'], ['ih', 't', 'w', 'aa', 'z', 'ah', 'k', 'y', 'uh', 'r', 'iy', 'ah', 's', 'k', 'ow', 'ih', 'n', 's', 'ih', 'd', 'ah', 'n', 's']]}\n"
     ]
    }
   ],
   "source": [
    "addPhonemesSpk(corp,'../data/lex.tgz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple_Eater', 'ryanjyoder', 'Perygryne', 'apdsqueaky', 'sharrington', 'yoyology', 'camdixon', 'Krellis', 'rocketman768', 'anonymous_9', 'anonymous_8', 'anonymous_5', 'anonymous_4', 'anonymous_7', 'anonymous_6', 'anonymous_1', 'anonymous_3', 'anonymous_2', 'ductapeguy', 'bhuvan', 'Primus', 'Q', 'adgar', 'thepinkcat', 'farmerjack', 'Steltek', 'TimS', 'pcsnpny']\n",
      "{'a0060': [array([ 278,  313,  139, ..., -443, -376, -179], dtype=int16), ['ANYWAY', 'NO', 'ONE', 'SAW', 'HER', 'LIKE', 'THAT'], ['eh', 'n', 'iy', 'w', 'ey', 'n', 'ow', 'w', 'ah', 'n', 's', 'ao', 'hh', 'er', 'l', 'ay', 'k', 'dh', 'ae', 't']], 'a0061': [array([-216, -258, -146, ..., -612, -567, -225], dtype=int16), ['PHILIP', 'SNATCHED', 'AT', 'THE', 'LETTER', 'WHICH', 'GREGSON', 'HELD', 'OUT', 'TO', 'HIM'], ['f', 'ih', 'l', 'ah', 'p', 's', 'n', 'ae', 'ch', 't', 'ae', 't', 'dh', 'ah', 'l', 'eh', 't', 'er', 'w', 'ih', 'ch', 'g', 'r', 'eh', 'g', 's', 'ah', 'n', 'hh', 'eh', 'l', 'd', 'aw', 't', 't', 'uw', 'hh', 'ih', 'm']], 'a0059': [array([ 566,  805,  615, ..., -226, -155,  -43], dtype=int16), ['HIS', 'IMMACULATE', 'APPEARANCE', 'WAS', 'GONE'], ['hh', 'ih', 'z', 'ih', 'm', 'ae', 'k', 'y', 'uw', 'l', 'ih', 't', 'ah', 'p', 'ih', 'r', 'ah', 'n', 's', 'w', 'aa', 'z', 'g', 'ao', 'n']], 'a0058': [array([ 99, 146, 251, ...,  21, 131,  64], dtype=int16), ['I', 'CAME', 'FOR', 'INFORMATION', 'MORE', 'OUT', 'OF', 'CURIOSITY', 'THAN', 'ANYTHING', 'ELSE'], ['ay', 'k', 'ey', 'm', 'f', 'ao', 'r', 'ih', 'n', 'f', 'ao', 'r', 'm', 'ey', 'sh', 'ah', 'n', 'm', 'ao', 'r', 'aw', 't', 'ah', 'v', 'k', 'y', 'uh', 'r', 'iy', 'aa', 's', 'ah', 't', 'iy', 'dh', 'ae', 'n', 'eh', 'n', 'iy', 'th', 'ih', 'ng', 'eh', 'l', 's']], 'a0057': [array([-432, -495, -253, ...,  259,  201,   32], dtype=int16), ['I', 'HAVE', 'NO', 'IDEA', 'REPLIED', 'PHILIP'], ['ay', 'hh', 'ae', 'v', 'n', 'ow', 'ay', 'd', 'iy', 'ah', 'r', 'ih', 'p', 'l', 'ay', 'd', 'f', 'ih', 'l', 'ah', 'p']], 'a0056': [array([-101,  -30,  113, ..., -406, -421, -314], dtype=int16), [\"PEARCE'S\", 'LITTLE', 'EYES', 'WERE', 'FIXED', 'ON', 'HIM', 'SHREWDLY'], ['p', 'ih', 'r', 's', 'ih', 'z', 'l', 'ih', 't', 'ah', 'l', 'ay', 'z', 'w', 'er', 'f', 'ih', 'k', 's', 't', 'aa', 'n', 'hh', 'ih', 'm', 'sh', 'r', 'uw', 'd', 'l', 'iy']], 'a0055': [array([ 953, 1215,  730, ...,   -3,  -62,  -60], dtype=int16), ['PHILIP', 'STOOD', 'UNDECIDED', 'HIS', 'EARS', 'STRAINED', 'TO', 'CATCH', 'THE', 'SLIGHTEST', 'SOUND'], ['f', 'ih', 'l', 'ah', 'p', 's', 't', 'uh', 'd', 'ah', 'n', 'd', 'ih', 's', 'ay', 'd', 'ih', 'd', 'hh', 'ih', 'z', 'ih', 'r', 'z', 's', 't', 'r', 'ey', 'n', 'd', 't', 'uw', 'k', 'ae', 'ch', 'dh', 'ah', 's', 'l', 'ay', 't', 'ah', 's', 't', 's', 'aw', 'n', 'd']], 'a0054': [array([304, 360, 264, ..., 395, 370, 143], dtype=int16), ['THERE', 'WAS', 'NOTHING', 'ON', 'THE', 'ROCK'], ['dh', 'eh', 'r', 'w', 'aa', 'z', 'n', 'ah', 'th', 'ih', 'ng', 'aa', 'n', 'dh', 'ah', 'r', 'aa', 'k']], 'a0053': [array([1209, 1442,  723, ...,  665,  726,  566], dtype=int16), ['SUDDENLY', 'HIS', 'FINGERS', 'CLOSED', 'TIGHTLY', 'OVER', 'THE', 'HANDKERCHIEF'], ['s', 'ah', 'd', 'ah', 'n', 'l', 'iy', 'hh', 'ih', 'z', 'f', 'ih', 'ng', 'g', 'er', 'z', 'k', 'l', 'ow', 'z', 'd', 't', 'ay', 't', 'l', 'iy', 'ow', 'v', 'er', 'dh', 'ah', 'hh', 'ae', 'ng', 'k', 'er', 'ch', 'ih', 'f']], 'a0052': [array([ -8,  78, 234, ..., 591, 420, 273], dtype=int16), ['IT', 'WAS', 'A', 'CURIOUS', 'COINCIDENCE'], ['ih', 't', 'w', 'aa', 'z', 'ah', 'k', 'y', 'uh', 'r', 'iy', 'ah', 's', 'k', 'ow', 'ih', 'n', 's', 'ih', 'd', 'ah', 'n', 's']]}\n"
     ]
    }
   ],
   "source": [
    "print corp.keys()\n",
    "\n",
    "spk=corp.keys()[0]\n",
    "\n",
    "print corp[spk]\n",
    "\n",
    "%xdel corp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aligned corpus\n",
    "\n",
    "As mentioned earlier, this sort or cropus has it's downsides. For one, we don't know when each phoneme occurs so we cannot train the system discriminatavely. While it's still possible, it would be nice if we could start with a simpler example. Another problem is choosing the right pronounciation variant mentioned above.\n",
    "\n",
    "To solve these issues, an automatic alignement was created using a different ASR system called [Kaldi](http://kaldi-asr.org). This system is a very good ASR solution that implements various types of models. It also contains simple out-of-the-box scripts for training on Voxforge data.\n",
    "\n",
    "To create the alignments using Kaldi, a working system had to be trained first and what's interesting, the same Voxforge data was used to train the system. How was this done? Well, Kaldi uses (among other things) a classic Gaussian Mixture Model and trains it using the EM algorithm. Initially the alignment is assumed to be even, throughout the file, but as the system is trained iteratively, the model gets better and thus the alignment gets more accurate. The system is trained with gradually better models to achieve even more accurate results and the provided solution here is generated using the \"tri3b\" model, as described in the scripts.\n",
    "\n",
    "The alignments in Kaldi are stored in special binary files, but there are simple tools to help convert them into something more easier to use. The type of file chosen for this example is the CTM file, which contains a series of lines in a text file, each line describing a single word or phoneme. The description has 5 columns: encoded file name, unused id (always 1), segment start, segment length and segment text (i.e. word of phoneme name/value). This file was generated using Kaldi, compressed using gzip and stored in 'ali.ctm.gz' in the 'data' directory of this project.\n",
    "\n",
    "Please note, that the number of files in this aligned set is smaller than the acutal count in the whole Voxforge dataset. This is because there is a small percentage of errors in the database (around a 100 files or so) and some recordings are of such poor quality that Kaldi couldn't generate a reasonable alignemnet for these files. We can simply ignore them here. This, however, doesn't mean that all the alignments present in the CTM are 100% accurate. There can still be mistakes there, but hopefully they are unlikely enough to not cause any issue.\n",
    "\n",
    "While this file contains everything that we need, it'd be useful to convert it into a datastructure that can be easily used in Python. The convertCTMToAli method is used for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading...\n",
      "Writing...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "convertCTMToAli('../data/ali.ctm.gz','../data/phones.list','../audio','../data/ali.pklz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the generated datastructure into a gzipped and pickled file, so we don't need to perform this more than once. This file is already included in the repository, so you can skip the step above.\n",
    "\n",
    "We can read the file like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "with gzip.open('../data/ali.pklz') as f:\n",
    "    ali=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59274\n",
      "Aaron\n",
      "[0, 10, 11, 28, 38, 23, 1, 31, 3, 23, 6, 25, 31, 3, 3, 35, 31, 28, 34, 32, 17, 23, 17, 31, 0]\n",
      "[480, 70, 70, 50, 70, 70, 90, 90, 30, 60, 170, 90, 90, 90, 80, 50, 120, 60, 80, 100, 40, 60, 80, 140, 720]\n",
      "Aaron-20080318-ngh\n",
      "b0346\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print len(ali)\n",
    "\n",
    "print ali[100].spk\n",
    "print ali[100].phones\n",
    "print ali[100].ph_lens\n",
    "print ali[100].archive\n",
    "print ali[100].audiofile\n",
    "print ali[100].data\n",
    "\n",
    "%xdel ali"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the audio data is not yet loaded at this step (it's set to None). To do this, we use the loadAlignedCorpus method. It loads the alignment and the appropriate audio datafile for each utterance. This step can take over half an hour to complete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 2531/3963 [33:14<03:20,  7.14it/s]"
     ]
    }
   ],
   "source": [
    "corp=loadAlignedCorpus('../data/ali.pklz','../audio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
